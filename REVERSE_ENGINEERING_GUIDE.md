# üîç DocuMentor Reverse Engineering & Improvement Guide

**Last Updated:** 2025-12-27
**Purpose:** Complete guide to understanding, analyzing, and improving DocuMentor

---

## üìä Project Statistics

- **Total Python Files:** 36
- **Lines of Code:** ~4,209 (rag_system package)
- **Classes & Functions:** 53+ across 14 core modules
- **Dependencies:** 27 packages
- **Architecture:** RAG (Retrieval-Augmented Generation) System

---

## üéØ Reverse Engineering Strategy

### Phase 1: Understanding the Foundation (Week 1)

#### 1.1 Dependency Graph Analysis

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  CORE DEPENDENCIES                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Web Frameworks
  ‚îú‚îÄ‚îÄ FastAPI (0.115.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ REST API Server
  ‚îú‚îÄ‚îÄ Uvicorn (0.32.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ASGI Server
  ‚îî‚îÄ‚îÄ Streamlit (1.29.0) ‚îÄ‚îÄ‚îÄ‚ñ∫ Web UI

Vector & Embeddings
  ‚îú‚îÄ‚îÄ ChromaDB (1.1.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Vector Database
  ‚îú‚îÄ‚îÄ sentence-transformers ‚ñ∫ Embeddings (all-MiniLM-L6-v2)
  ‚îî‚îÄ‚îÄ LangChain (0.3.27) ‚îÄ‚îÄ‚îÄ‚ñ∫ RAG Framework

LLM Providers
  ‚îú‚îÄ‚îÄ Ollama (HTTP API) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Local LLM
  ‚îú‚îÄ‚îÄ OpenAI (1.0.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ GPT API
  ‚îî‚îÄ‚îÄ Google Gemini (0.3.0) ‚ñ∫ Gemini API

Document Processing
  ‚îú‚îÄ‚îÄ pypdf (6.0.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ PDF Extraction
  ‚îú‚îÄ‚îÄ python-docx (1.1.0) ‚îÄ‚îÄ‚ñ∫ Word Files
  ‚îú‚îÄ‚îÄ openpyxl (3.1.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Excel
  ‚îî‚îÄ‚îÄ python-pptx (0.6.0) ‚îÄ‚îÄ‚ñ∫ PowerPoint

Infrastructure
  ‚îú‚îÄ‚îÄ Pydantic (2.11.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Validation & Config
  ‚îú‚îÄ‚îÄ prometheus-client ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Metrics
  ‚îú‚îÄ‚îÄ slowapi (0.1.9) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Rate Limiting
  ‚îî‚îÄ‚îÄ filelock (3.13.0) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Concurrency Control
```

**Key Insights:**
- **No single point of failure** - Multi-provider LLM support
- **Heavy reliance on ChromaDB** - Critical for performance
- **Caching at multiple levels** - Embeddings + Responses
- **Security through middleware** - Auth + Validation layers

---

#### 1.2 Module Dependency Map

```
Entry Points
    ‚îú‚îÄ‚îÄ launcher.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Orchestrates API + UI
    ‚îú‚îÄ‚îÄ api_server.py ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ FastAPI standalone
    ‚îî‚îÄ‚îÄ main.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Streamlit standalone

         ‚Üì

API Layer (rag_system/api/)
    ‚îú‚îÄ‚îÄ server.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Route definitions
    ‚îî‚îÄ‚îÄ middleware/
        ‚îú‚îÄ‚îÄ auth.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ API key verification
        ‚îî‚îÄ‚îÄ validation.py ‚ñ∫ Input sanitization

         ‚Üì

Core Logic (rag_system/core/)
    ‚îú‚îÄ‚îÄ chunking/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ DocumentChunker
    ‚îú‚îÄ‚îÄ retrieval/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ChromaVectorStore
    ‚îú‚îÄ‚îÄ generation/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LLMService (3 providers)
    ‚îú‚îÄ‚îÄ processing/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ DocumentProcessor
    ‚îú‚îÄ‚îÄ search/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ WebSearchProvider
    ‚îî‚îÄ‚îÄ utils/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Cache, Metrics, Logger

         ‚Üì

Configuration (rag_system/config/)
    ‚îî‚îÄ‚îÄ settings.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Pydantic Settings (.env)

         ‚Üì

Data Storage (data/)
    ‚îú‚îÄ‚îÄ chroma_db/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Persistent vectors
    ‚îú‚îÄ‚îÄ cache/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Response cache
    ‚îî‚îÄ‚îÄ embeddings_cache/ ‚ñ∫ Embedding cache
```

---

### Phase 2: Deep Dive into Components

#### 2.1 Critical Code Paths to Study

**Priority 1: Core RAG Pipeline**
1. `rag_system/core/retrieval/vector_store.py` (387 lines)
   - Study: ChromaDB initialization with file locking
   - Study: Embedding caching layer
   - Study: Retry logic and error handling

2. `rag_system/core/generation/llm_handler.py` (299 lines)
   - Study: Multi-provider abstraction (Ollama, OpenAI, Gemini)
   - Study: Provider fallback mechanism
   - Study: Code generation vs Q&A modes

3. `rag_system/core/chunking/chunker.py` (375 lines)
   - Study: Intelligent chunking (code-aware, markdown-aware)
   - Study: Parallel processing with ThreadPoolExecutor
   - Study: Metadata preservation

**Priority 2: API & Security**
4. `rag_system/api/server.py`
   - Study: FastAPI route structure
   - Study: Request/Response models (Pydantic)
   - Study: Rate limiting integration

5. `rag_system/api/middleware/auth.py`
   - Study: Timing-safe comparison for API keys
   - Study: Security best practices

6. `rag_system/api/middleware/validation.py`
   - Study: Input validation strategies
   - Study: File upload security (MIME type checking)

**Priority 3: Performance & Monitoring**
7. `rag_system/core/utils/cache.py`
   - Study: LRU cache implementation
   - Study: Cache invalidation strategy

8. `rag_system/core/utils/metrics.py`
   - Study: Prometheus metrics integration
   - Study: Performance tracking

---

#### 2.2 Data Flow Mapping Exercise

**Exercise 1: Trace a Query from API to Response**

```python
# Follow this path through the code:
1. POST /ask {"question": "How to use FastAPI?"}
   ‚Üí File: rag_system/api/server.py:ask_question()

2. Middleware chain:
   ‚Üí auth.py: verify_api_key()
   ‚Üí validation.py: validate_query()
   ‚Üí Rate limiter check

3. Vector search:
   ‚Üí vector_store.py: search()
   ‚Üí Check embedding cache
   ‚Üí Query ChromaDB
   ‚Üí Return top-k results

4. LLM generation:
   ‚Üí llm_handler.py: generate_answer()
   ‚Üí Build context from top-3 results
   ‚Üí Call Ollama/OpenAI/Gemini
   ‚Üí Parse response

5. Response caching:
   ‚Üí cache.py: set()
   ‚Üí Save to JSON file

6. Return response to client
```

**Exercise 2: Trace a Document Upload**

```python
1. POST /upload (multipart/form-data)
   ‚Üí server.py: upload_document()

2. File validation:
   ‚Üí validation.py: validate_file_upload()
   ‚Üí Check MIME type (python-magic)
   ‚Üí Check size limit (50MB)

3. Document processing:
   ‚Üí document_processor.py: process_document()
   ‚Üí Extract text based on format (PDF/DOCX/etc.)

4. Chunking:
   ‚Üí chunker.py: chunk_documents()
   ‚Üí Parallel chunking (ThreadPoolExecutor)
   ‚Üí Generate metadata

5. Embedding & storage:
   ‚Üí vector_store.py: add_documents()
   ‚Üí Generate embeddings (sentence-transformers)
   ‚Üí Cache embeddings
   ‚Üí Upsert to ChromaDB
```

---

### Phase 3: Identify Improvement Opportunities

#### 3.1 Performance Bottlenecks

**Current Analysis:**

| Component | Potential Bottleneck | Evidence |
|-----------|---------------------|----------|
| **ChromaDB Operations** | File locking blocks concurrent reads | `FileLock` with 10s timeout |
| **Embedding Generation** | CPU-intensive, no GPU support | sentence-transformers on CPU |
| **LLM Calls** | 120s timeout, synchronous | No async/await in Ollama calls |
| **Chunking** | Limited parallelism (4 workers) | `MAX_WORKERS=4` default |
| **Caching** | JSON-based, slow for large responses | File I/O for every cache hit |

**Recommended Improvements:**

1. **Replace File Locking with Read-Write Locks**
   ```python
   # Current: Exclusive lock for reads & writes
   with self.lock:  # Blocks everything
       results = self.collection.query()

   # Improved: Allow concurrent reads
   from threading import RLock
   # Use RLock or implement read-write lock pattern
   ```

2. **Add Async Support**
   ```python
   # Current: Synchronous Ollama calls
   response = requests.post(...)

   # Improved: Use httpx for async
   async with httpx.AsyncClient() as client:
       response = await client.post(...)
   ```

3. **GPU Acceleration for Embeddings**
   ```python
   # Add to vector_store.py
   device = "cuda" if torch.cuda.is_available() else "cpu"
   model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
   ```

4. **Redis-based Caching**
   ```python
   # Replace JSON file cache with Redis
   import redis
   cache = redis.Redis(host='localhost', port=6379, db=0)
   ```

5. **Batch Processing Optimization**
   ```python
   # Current: Fixed batch sizes
   BATCH_SIZE = 100

   # Improved: Dynamic batching based on available memory
   import psutil
   mem = psutil.virtual_memory()
   batch_size = min(200, max(25, int(mem.available / (1024**2) / 10)))
   ```

---

#### 3.2 Code Quality Issues

**Issue 1: Inconsistent Error Handling**
```python
# Problem: Different error handling patterns
# File: vector_store.py
try:
    self.collection = self.client.get_collection(name=self.collection_name)
except ValueError:  # Too specific
    ...

# File: llm_handler.py
except Exception as e:  # Too broad
    logger.error(f"Error: {e}")
```

**Solution:**
```python
# Create custom exceptions
class VectorStoreError(Exception):
    """Base exception for vector store operations"""
    pass

class EmbeddingError(VectorStoreError):
    """Embedding generation failed"""
    pass

# Use consistently
try:
    embeddings = self.generate_embeddings(texts)
except EmbeddingError as e:
    logger.error(f"Embedding failed: {e}")
    raise
```

**Issue 2: Missing Type Hints**
```python
# Current: Incomplete type hints
def search(self, query, k=5):  # Missing return type
    ...

# Improved: Full type annotations
from typing import List, Dict, Any

def search(
    self,
    query: str,
    k: int = 5
) -> List[Dict[str, Any]]:
    ...
```

**Issue 3: Magic Numbers & Strings**
```python
# Current: Hard-coded values scattered
context_text += f"\n{i}. From '{source}':\n{content[:500]}...\n"
                                              # ^^^^ Magic number

# Improved: Use constants
from rag_system.core.constants import MAX_CONTEXT_LENGTH

context_text += f"\n{i}. From '{source}':\n{content[:MAX_CONTEXT_LENGTH]}...\n"
```

---

#### 3.3 Security Improvements

**Issue 1: API Key Storage**
```python
# Current: API key in .env file (plain text)
API_KEY=my-secret-key

# Improved: Use secrets management
# - AWS Secrets Manager
# - HashiCorp Vault
# - Environment-specific encrypted secrets
```

**Issue 2: Rate Limiting Per User**
```python
# Current: Rate limiting by IP address
limiter = Limiter(key_func=get_remote_address)

# Improved: Rate limiting by API key
def get_api_key(request: Request) -> str:
    return request.headers.get("X-API-Key", "anonymous")

limiter = Limiter(key_func=get_api_key)
```

**Issue 3: Input Sanitization**
```python
# Add SQL injection prevention for metadata queries
# Add XSS prevention for web UI
from bleach import clean

def sanitize_user_input(text: str) -> str:
    return clean(text, tags=[], strip=True)
```

---

#### 3.4 Testing Gaps

**Current Test Coverage:**
```
tests/
‚îú‚îÄ‚îÄ test_auth.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ API authentication
‚îú‚îÄ‚îÄ test_cache.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Caching system
‚îú‚îÄ‚îÄ test_validation.py ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Input validation
‚îî‚îÄ‚îÄ conftest.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Shared fixtures
```

**Missing Tests:**
1. ‚ùå **Integration tests** for full RAG pipeline
2. ‚ùå **Load tests** for concurrent requests
3. ‚ùå **Unit tests** for chunking logic
4. ‚ùå **Mock tests** for LLM providers
5. ‚ùå **Security tests** (penetration testing)

**Recommended Test Additions:**
```python
# tests/test_rag_pipeline.py
@pytest.mark.integration
async def test_full_query_pipeline():
    """Test complete flow from query to response"""
    # Upload document
    # Query document
    # Verify response quality
    # Check caching works
    pass

# tests/test_performance.py
@pytest.mark.performance
async def test_concurrent_queries(benchmark):
    """Test 100 concurrent queries"""
    # Use pytest-benchmark
    # Measure p95, p99 latency
    pass

# tests/test_chunking.py
def test_code_aware_chunking():
    """Verify code blocks are preserved"""
    code_doc = "```python\ndef foo():\n    pass\n```"
    chunks = chunker.chunk_text(code_doc)
    assert "```python" in chunks[0]
```

---

### Phase 4: Architectural Improvements

#### 4.1 Microservices Architecture (Advanced)

**Current:** Monolithic application (API + UI + RAG)

**Proposed:** Service-oriented architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              API Gateway                     ‚îÇ
‚îÇ         (FastAPI + Auth + Rate Limit)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ          ‚îÇ
    ‚Üì         ‚Üì          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇEmbedding‚îÇ ‚îÇ  LLM   ‚îÇ ‚îÇ Search ‚îÇ
‚îÇ Service ‚îÇ ‚îÇService ‚îÇ ‚îÇService ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ         ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇVector DB ‚îÇ
        ‚îÇ(ChromaDB)‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits:**
- Independent scaling
- Technology flexibility (can replace LLM service without affecting others)
- Easier testing
- Better fault isolation

---

#### 4.2 Event-Driven Architecture

**Current:** Synchronous request-response

**Proposed:** Event-driven with message queue

```python
# Use RabbitMQ or Redis Pub/Sub for async processing

# Producer (API)
@app.post("/upload")
async def upload_document(file: UploadFile):
    # Save file
    # Publish event: "document.uploaded"
    await event_bus.publish("document.uploaded", {
        "file_id": file_id,
        "filename": filename
    })
    return {"status": "processing", "job_id": job_id}

# Consumer (Worker)
@event_bus.subscribe("document.uploaded")
async def process_document(event):
    # Process in background
    # Chunk document
    # Generate embeddings
    # Store in vector DB
    # Publish: "document.indexed"
```

**Benefits:**
- Non-blocking uploads
- Better user experience
- Horizontal scaling of workers
- Retry failed jobs

---

#### 4.3 Caching Strategy Improvement

**Current:** Single-layer cache (JSON files)

**Proposed:** Multi-tier caching

```
Level 1: In-Memory Cache (LRU, 100 items, 1-minute TTL)
    ‚Üì (cache miss)
Level 2: Redis Cache (10,000 items, 1-hour TTL)
    ‚Üì (cache miss)
Level 3: Vector DB Query
```

**Implementation:**
```python
class MultiTierCache:
    def __init__(self):
        self.l1_cache = LRUCache(max_size=100, ttl=60)
        self.l2_cache = redis.Redis(host='localhost', port=6379)
        self.l3_backend = vector_store

    async def get(self, key: str):
        # L1: Memory
        if result := self.l1_cache.get(key):
            return result

        # L2: Redis
        if result := await self.l2_cache.get(key):
            self.l1_cache.set(key, result)
            return result

        # L3: Vector DB
        result = await self.l3_backend.search(key)
        await self.l2_cache.setex(key, 3600, result)
        self.l1_cache.set(key, result)
        return result
```

---

### Phase 5: Development Workflow

#### 5.1 Setting Up Development Environment

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 2. Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Create this!

# 3. Install development tools
pip install pytest pytest-asyncio pytest-cov black flake8 mypy pre-commit

# 4. Set up pre-commit hooks
cat > .pre-commit-config.yaml << EOF
repos:
  - repo: https://github.com/psf/black
    rev: 23.9.1
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 6.1.0
    hooks:
      - id: flake8
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.5.1
    hooks:
      - id: mypy
EOF

pre-commit install

# 5. Set up environment
cp .env.example .env
# Edit .env with your settings

# 6. Initialize database
python -c "from rag_system.core.retrieval.vector_store import ChromaVectorStore; ChromaVectorStore()"
```

---

#### 5.2 Code Analysis Tools

**Install Analysis Tools:**
```bash
pip install pylint bandit radon pytest-cov

# 1. Linting (code quality)
pylint rag_system/

# 2. Security analysis
bandit -r rag_system/

# 3. Complexity analysis
radon cc rag_system/ -a -nb

# 4. Test coverage
pytest --cov=rag_system --cov-report=html
```

**Expected Output Analysis:**
```
# Cyclomatic Complexity (radon cc)
A = Simple (1-5)    ‚úÖ Target: 80% of functions
B = Medium (6-10)   ‚ö†Ô∏è  Acceptable: 15% of functions
C = Complex (11-20) ‚ùå Refactor: < 5% of functions
D+ = Very Complex   ‚ùå Critical: 0% of functions

# Coverage Goals
Overall: > 80%
Core modules: > 90%
Utils: > 95%
```

---

#### 5.3 Systematic Improvement Workflow

**Step-by-Step Process:**

```bash
# 1. Create feature branch
git checkout -b improve/performance-optimization

# 2. Analyze current state
pytest --cov=rag_system
radon cc rag_system/core/retrieval/vector_store.py

# 3. Make targeted improvements
# - One improvement at a time
# - Write tests first (TDD)
# - Refactor incrementally

# 4. Verify improvements
pytest --cov=rag_system
python -m pytest tests/ -v -s

# 5. Benchmark (before/after)
python -m pytest tests/test_performance.py --benchmark-only

# 6. Commit changes
git add .
git commit -m "perf: optimize vector search with async queries

- Add async support to ChromaVectorStore.search()
- Implement connection pooling
- Results: 40% faster query time (benchmark included)

Closes #123"

# 7. Push and create PR
git push -u origin improve/performance-optimization
```

---

### Phase 6: Monitoring & Observability

#### 6.1 Enhanced Metrics

**Add to `metrics.py`:**
```python
from prometheus_client import Histogram, Gauge

# Query latency by provider
llm_latency_by_provider = Histogram(
    'llm_latency_by_provider_seconds',
    'LLM response time by provider',
    ['provider', 'model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

# Cache hit rate
cache_hit_rate = Gauge(
    'cache_hit_rate',
    'Percentage of cache hits',
    ['cache_type']
)

# Vector DB size
vector_db_size = Gauge(
    'vector_db_total_chunks',
    'Total number of chunks in vector DB'
)

# Active users
active_users = Gauge(
    'active_users_total',
    'Number of unique active users in last 5 minutes'
)
```

---

#### 6.2 Structured Logging

**Enhance `logger.py`:**
```python
import structlog

# Configure structured logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()

# Usage
logger.info(
    "vector_search_completed",
    query="How to use FastAPI",
    results_count=8,
    duration_ms=150,
    cache_hit=False,
    user_id="user_123"
)

# Output (JSON):
# {
#   "event": "vector_search_completed",
#   "query": "How to use FastAPI",
#   "results_count": 8,
#   "duration_ms": 150,
#   "cache_hit": false,
#   "user_id": "user_123",
#   "timestamp": "2025-12-27T10:30:45.123Z",
#   "level": "info"
# }
```

---

### Phase 7: Documentation Improvements

#### 7.1 API Documentation

**Enhance OpenAPI schema in `server.py`:**
```python
app = FastAPI(
    title="DocuMentor API",
    description="""
    ü§ñ Advanced RAG System for Documentation Queries

    ## Features
    - Multi-provider LLM support (Ollama, OpenAI, Gemini)
    - Semantic search with ChromaDB
    - Code generation capabilities
    - Technology-specific filtering

    ## Authentication
    Include your API key in the `X-API-Key` header.

    ## Rate Limits
    - /ask: 30 requests/minute
    - /upload: 10 requests/minute
    - /search: 60 requests/minute
    """,
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=[
        {
            "name": "queries",
            "description": "Query operations for asking questions"
        },
        {
            "name": "uploads",
            "description": "Document upload and processing"
        },
        {
            "name": "monitoring",
            "description": "Health checks and metrics"
        }
    ]
)

@app.post(
    "/ask",
    response_model=QueryResponse,
    tags=["queries"],
    summary="Ask a question",
    description="""
    Submit a question and receive an AI-generated answer with sources.

    **Example Request:**
    ```json
    {
      "question": "How to create a FastAPI endpoint?",
      "search_k": 8,
      "technology_filter": "FastAPI"
    }
    ```

    **Response includes:**
    - AI-generated answer
    - Source documents with metadata
    - Response time and provider used
    """
)
async def ask_question(request: QueryRequest):
    ...
```

---

#### 7.2 Code Documentation

**Add comprehensive docstrings:**
```python
class ChromaVectorStore:
    """
    Vector store implementation using ChromaDB for document storage and retrieval.

    This class provides a thread-safe interface to ChromaDB with the following features:
    - File-based locking for concurrent access safety
    - Embedding caching to improve performance
    - Automatic retry logic for transient failures
    - Batch processing for large document sets

    Example:
        >>> store = ChromaVectorStore()
        >>> store.add_documents(
        ...     texts=["Hello world"],
        ...     metadatas=[{"source": "test.txt"}],
        ...     ids=["doc1"]
        ... )
        >>> results = store.search("greeting", k=5)

    Attributes:
        persist_directory (str): Path to ChromaDB storage directory
        collection_name (str): Name of the ChromaDB collection
        lock (FileLock): File lock for concurrent access control
        client (chromadb.PersistentClient): ChromaDB client instance
        collection (chromadb.Collection): Active collection instance
        embedding_function (EmbeddingService): Embedding generation service

    Thread Safety:
        All public methods use file-based locking to ensure thread safety.
        Multiple processes can safely access the same ChromaDB instance.

    Performance:
        - Embedding caching reduces duplicate computation by ~80%
        - Batch processing handles 1000+ documents efficiently
        - Automatic retry logic handles transient ChromaDB failures
    """

    def search(
        self,
        query: str,
        k: int = 5,
        technology: Optional[str] = None,
        source_filter: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        Perform semantic search for documents similar to the query.

        Args:
            query: The search query text
            k: Number of results to return (default: 5, max: 50)
            technology: Optional technology filter (e.g., "Python", "FastAPI")
            source_filter: Optional list of source files to filter by

        Returns:
            List of result dictionaries, each containing:
                - content (str): Document text content
                - metadata (dict): Document metadata (source, title, etc.)
                - score (float): Similarity score (0.0 to 1.0)

        Raises:
            ValueError: If k is invalid or query is empty
            VectorStoreError: If ChromaDB operation fails after retries

        Example:
            >>> results = store.search(
            ...     query="How to handle errors",
            ...     k=5,
            ...     technology="Python"
            ... )
            >>> for result in results:
            ...     print(f"Score: {result['score']:.2f}")
            ...     print(f"Content: {result['content'][:100]}")

        Performance:
            - Typical query time: 50-200ms
            - Cached embeddings: ~30ms
            - First query (cold start): ~500ms
        """
        ...
```

---

## üöÄ Quick Start Improvement Checklist

### Week 1: Foundation
- [ ] Set up development environment with all tools
- [ ] Run code analysis (pylint, bandit, radon)
- [ ] Review current test coverage
- [ ] Create baseline performance benchmarks
- [ ] Read through all core modules (priority 1 files)

### Week 2: Performance
- [ ] Implement async support for LLM calls
- [ ] Add GPU support for embeddings
- [ ] Optimize ChromaDB locking strategy
- [ ] Implement multi-tier caching
- [ ] Create performance benchmarks

### Week 3: Code Quality
- [ ] Add comprehensive type hints
- [ ] Implement custom exceptions
- [ ] Refactor complex functions (radon CC > 10)
- [ ] Add missing docstrings
- [ ] Set up pre-commit hooks

### Week 4: Testing
- [ ] Increase unit test coverage to 80%
- [ ] Add integration tests for RAG pipeline
- [ ] Add load tests for concurrent requests
- [ ] Add security tests (bandit + manual)
- [ ] Set up CI/CD pipeline

### Week 5: Features
- [ ] Implement event-driven document processing
- [ ] Add Redis caching layer
- [ ] Improve error handling consistency
- [ ] Add request tracing (correlation IDs)
- [ ] Enhance monitoring & alerts

### Week 6: Documentation
- [ ] Update API documentation
- [ ] Add code examples for all endpoints
- [ ] Create architecture diagrams
- [ ] Write deployment guide
- [ ] Create troubleshooting guide

---

## üìö Learning Resources

### Understanding RAG Systems
1. **LangChain Documentation**: https://python.langchain.com/docs/
2. **ChromaDB Guides**: https://docs.trychroma.com/
3. **Sentence Transformers**: https://www.sbert.net/

### FastAPI & Python
1. **FastAPI Best Practices**: https://github.com/zhanymkanov/fastapi-best-practices
2. **Python Type Hints**: https://mypy.readthedocs.io/
3. **Async Python**: https://realpython.com/async-io-python/

### Performance Optimization
1. **Python Performance Tips**: https://wiki.python.org/moin/PythonSpeed
2. **Database Optimization**: ChromaDB performance tuning
3. **Caching Strategies**: Redis patterns

---

## üéØ Success Metrics

Track these metrics to measure improvements:

| Metric | Current | Target | How to Measure |
|--------|---------|--------|----------------|
| Test Coverage | Unknown | > 80% | `pytest --cov` |
| API Response Time (p95) | Unknown | < 500ms | Prometheus metrics |
| Code Complexity (Avg) | Unknown | < 6 | `radon cc` |
| Security Issues | Unknown | 0 critical | `bandit -r` |
| Documentation Coverage | ~40% | > 90% | Manual review |
| Cache Hit Rate | Unknown | > 70% | Custom metrics |

---

## üîß Troubleshooting Common Issues

### Issue: ChromaDB Lock Timeout
**Symptom:** `Timeout acquiring lock on chroma_db`
**Solution:** Increase lock timeout or implement read-write locks

### Issue: Slow Embedding Generation
**Symptom:** Embeddings take > 1 second
**Solution:** Enable GPU support, use smaller model, or increase caching

### Issue: High Memory Usage
**Symptom:** Process uses > 4GB RAM
**Solution:** Reduce batch sizes, enable garbage collection, use generators

---

## üìù Contribution Guidelines

When making improvements:

1. **One improvement at a time** - Don't mix refactoring with features
2. **Tests first** - Write tests before code (TDD)
3. **Benchmark** - Measure performance before and after
4. **Document** - Update docs with every change
5. **Review** - Get code review before merging

---

**Ready to start? Begin with Phase 1.1 and work systematically through each phase!**
