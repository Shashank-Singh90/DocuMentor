# üöÄ DocuMentor Improvement Implementation Roadmap

**Generated:** 2025-12-27
**Based on:** Comprehensive code analysis of DocuMentor v2.0.0

---

## üìä Analysis Summary

**Total Issues Found:** 41
- **High Priority (Security & Critical Bugs):** 7 issues
- **Medium Priority (Performance & Reliability):** 30 issues
- **Low Priority (Technical Debt):** 4 issues

**Estimated Impact:**
- Performance improvement: 30-50% faster responses
- Security hardening: Fix 10+ vulnerabilities
- Code quality: 80%+ test coverage
- Maintainability: Reduce complexity by 25%

---

## üéØ Week-by-Week Implementation Plan

### Week 1: Critical Security & Bug Fixes (HIGH PRIORITY)

**Goal:** Fix security vulnerabilities and critical bugs that could crash the system

#### Day 1-2: Input Validation & Security

**Tasks:**
1. Add input validation to vector store
2. Add file upload size limits
3. Add query sanitization
4. Fix undefined references

**Estimated Time:** 6-8 hours

**Commands:**
```bash
# Create feature branch
git checkout -b fix/security-hardening

# Make changes (detailed below)
# Run tests
pytest tests/ -v

# Commit
git add .
git commit -m "fix: add critical security validations

- Add input length validation to vector_store
- Add file size limits to upload endpoint
- Sanitize user queries to prevent injection
- Fix undefined references in server.py

Fixes: #security-audit-2025"

git push -u origin fix/security-hardening
```

**Files to Modify:**

**1. rag_system/core/retrieval/vector_store.py** (Add after line 79)
```python
def add_documents(self, texts: List[str], metadatas: List[dict], ids: List[str]):
    """Add documents with input validation"""
    # ‚≠ê ADD THIS VALIDATION
    if not texts or not metadatas or not ids:
        raise ValueError("texts, metadatas, and ids cannot be empty")

    if not (len(texts) == len(metadatas) == len(ids)):
        raise ValueError(
            f"Length mismatch: texts={len(texts)}, "
            f"metadatas={len(metadatas)}, ids={len(ids)}"
        )

    if len(set(ids)) != len(ids):
        raise ValueError("IDs must be unique")

    logger.info(f"Adding {len(texts)} validated documents")

    # ... rest of existing method
```

**2. rag_system/api/server.py** (Add before line 246)
```python
import re

def sanitize_query(query: str) -> str:
    """Sanitize user query to prevent injection attacks"""
    # Remove null bytes
    query = query.replace('\x00', '')
    # Limit length
    query = query[:2000]
    # Remove potentially dangerous characters
    query = re.sub(r'[<>\"\'%;()&+]', '', query)
    return query.strip()

# Then modify the /ask endpoint (around line 246):
@app.post("/ask", response_model=QueryResponse, tags=["Queries"])
async def ask_question(request: QueryRequest):
    # ‚≠ê SANITIZE INPUT
    clean_query = sanitize_query(request.question)

    # Use clean_query instead of request.question
    search_results = vector_store.search(clean_query, k=request.search_k, ...)
    # ... rest
```

**3. rag_system/api/server.py** (Modify upload endpoint around line 369)
```python
@app.post("/upload", tags=["Documents"])
async def upload_document(
    file: UploadFile = File(...),
    source: str = Form(default="api_upload")
):
    # ‚≠ê ADD SIZE LIMIT CHECK
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

    content = b""
    total_size = 0

    while chunk := await file.read(8192):
        total_size += len(chunk)
        if total_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Max size: {MAX_FILE_SIZE/1024/1024:.0f}MB"
            )
        content += chunk

    # Reset file pointer
    await file.seek(0)

    # ... rest of existing code
```

**4. rag_system/api/server.py** (Remove duplicate endpoint)
```python
# ‚≠ê DELETE LINES 363-367 (duplicate /ask endpoint)
# Search for the second occurrence of:
# @app.post("/ask"...)
# and DELETE it completely
```

#### Day 3: Fix Critical Bugs

**Files to Modify:**

**5. rag_system/core/retrieval/vector_store.py** (Fix line 149)
```python
# REPLACE the malformed try-except (lines 130-152) with:
for retry in range(CHROMADB_RETRY_ATTEMPTS):
    try:
        self.collection.upsert(
            documents=clean_texts,
            metadatas=clean_metadatas,
            ids=ids[i:i+batch_size]
        )
        added += len(clean_texts)
        break
    except Exception as e:
        error_msg = str(e).lower()

        # Don't retry quota errors
        if "quota" in error_msg or "limit" in error_msg:
            logger.error(f"ChromaDB quota exceeded: {e}")
            raise

        # Retry on other errors
        if retry < CHROMADB_RETRY_ATTEMPTS - 1:
            wait_time = CHROMADB_RETRY_DELAY * (2 ** retry)
            logger.warning(
                f"Retry {retry + 1}/{CHROMADB_RETRY_ATTEMPTS} "
                f"after {wait_time}s: {e}"
            )
            time.sleep(wait_time)
        else:
            logger.error(f"Failed batch after all retries: {e}")
            break
```

**Commit and Push:**
```bash
pytest tests/ -v
git add .
git commit -m "fix: resolve critical bugs in vector store and API

- Fix malformed exception handling in ChromaDB retry logic
- Remove duplicate /ask endpoint definition
- Improve error messages for debugging"

git push
```

---

### Week 2: Performance Optimizations (MEDIUM PRIORITY)

**Goal:** Improve response time by 30-50%

#### Day 1-2: Add Connection Pooling & Async Support

**Files to Modify:**

**1. rag_system/core/generation/llm_handler.py** (Modify OllamaProvider)
```python
class OllamaProvider(BaseLLMProvider):
    def __init__(self):
        self.base_url = f"http://{settings.ollama_host}"
        self.model = settings.ollama_model

        # ‚≠ê ADD CONNECTION POOLING
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=requests.adapters.Retry(
                total=3,
                backoff_factor=0.3,
                status_forcelist=[500, 502, 503, 504]
            )
        )
        self.session.mount('http://', adapter)
        logger.info("Initialized Ollama with connection pooling")

    def generate_response(self, prompt: str, context: List[Dict]) -> str:
        # ... existing code ...

        # ‚≠ê REPLACE: response = requests.post(...)
        # WITH:
        response = self.session.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": full_prompt,
                "stream": False
            },
            timeout=settings.ollama_timeout
        )

        # ... rest unchanged
```

**2. requirements.txt** (Add async support)
```bash
# ‚≠ê ADD at the end:
httpx~=0.28.0  # For async HTTP if not already present
aiofiles~=24.1.0  # For async file operations
```

**Commit:**
```bash
pip install -r requirements.txt
pytest tests/ -v

git add .
git commit -m "perf: add HTTP connection pooling to Ollama provider

- Add requests.Session with connection pooling (10 connections)
- Add automatic retries for transient errors
- Expected improvement: 20-30% faster for sequential requests

Benchmark: TBD after load testing"

git push
```

#### Day 3-4: Optimize Caching

**Files to Modify:**

**1. rag_system/core/retrieval/vector_store.py** (Add cache to get_collection_stats)
```python
import time

class ChromaVectorStore:
    def __init__(self):
        # ... existing init code ...

        # ‚≠ê ADD cache attributes
        self._stats_cache = None
        self._stats_cache_time = 0
        self._stats_cache_ttl = 60  # 60 seconds

    def get_collection_stats(self, force_refresh: bool = False) -> Dict:
        """Get collection statistics with 60-second cache"""

        # ‚≠ê ADD cache check
        if not force_refresh and self._stats_cache is not None:
            age = time.time() - self._stats_cache_time
            if age < self._stats_cache_ttl:
                logger.debug(f"Returning cached stats (age: {age:.1f}s)")
                return self._stats_cache

        # Compute stats (existing code)
        total_docs = self.collection.count()

        # ... existing stats computation ...

        stats = {
            "total_documents": total_docs,
            "source_distribution": source_distribution
        }

        # ‚≠ê UPDATE cache
        self._stats_cache = stats
        self._stats_cache_time = time.time()

        return stats
```

**2. rag_system/api/server.py** (Cache /technologies endpoint)
```python
from functools import lru_cache
import time

# ‚≠ê ADD cache decorator with TTL
_tech_cache = None
_tech_cache_time = 0

@app.get("/technologies", tags=["Technologies"])
async def list_technologies():
    global _tech_cache, _tech_cache_time

    # ‚≠ê CHECK cache (5 minutes)
    if _tech_cache and (time.time() - _tech_cache_time) < 300:
        logger.debug("Returning cached technologies")
        return _tech_cache

    # ... existing technology gathering code ...

    result = {"technologies": tech_list}

    # ‚≠ê UPDATE cache
    _tech_cache = result
    _tech_cache_time = time.time()

    return result
```

**Commit:**
```bash
pytest tests/ -v

git add .
git commit -m "perf: add multi-level caching for stats and technologies

- Cache collection stats for 60 seconds
- Cache /technologies endpoint for 5 minutes
- Reduces database queries by ~90% for repeated stats calls

Expected improvement: Sub-100ms for cached requests"

git push
```

#### Day 5: Optimize Batch Processing

**File:** rag_system/core/retrieval/vector_store.py

```python
def _calculate_optimal_batch_size(self, texts: List[str]) -> int:
    """Calculate optimal batch size with better sampling"""
    if not texts:
        return 100

    # ‚≠ê USE random sampling instead of first 100
    import random
    sample_size = min(100, len(texts))
    sample = random.sample(texts, sample_size) if len(texts) > sample_size else texts

    avg_length = sum(len(text) for text in sample) / len(sample)

    # Rest unchanged...
```

**Commit:**
```bash
git add .
git commit -m "perf: improve batch size calculation with random sampling

- Use random sampling instead of first 100 documents
- Prevents bias from ordered document sets
- More accurate batch size estimation"

git push
```

---

### Week 3: Code Quality & Testing (MEDIUM PRIORITY)

**Goal:** Achieve 80%+ test coverage and reduce code complexity

#### Day 1-2: Refactor Duplicate Code

**1. rag_system/core/generation/llm_handler.py** (Extract common context building)
```python
class BaseLLMProvider(ABC):
    """Base class for LLM providers"""

    # ‚≠ê ADD helper method
    def _build_context(
        self,
        context: List[Dict],
        max_results: int = 3,
        max_length: int = 800
    ) -> str:
        """Build formatted context from search results"""
        if not context:
            return ""

        context_text = "Context from documents:\n"
        for i, result in enumerate(context[:max_results], 1):
            content = result.get('content', '')[:max_length]
            source = result.get('metadata', {}).get('title', 'Unknown')
            context_text += f"\n{i}. From '{source}':\n{content}...\n"

        return context_text

    @abstractmethod
    def generate_response(self, prompt: str, context: List[Dict]) -> str:
        pass

    @abstractmethod
    def is_available(self) -> bool:
        pass

# ‚≠ê UPDATE each provider to use helper:
class OllamaProvider(BaseLLMProvider):
    def generate_response(self, prompt: str, context: List[Dict]) -> str:
        try:
            # Use helper instead of duplicated code
            context_text = self._build_context(context, max_results=3, max_length=800)

            full_prompt = f"{context_text}\n\nQuestion: {prompt}\n\nAnswer:"

            # ... rest unchanged
```

**Do the same for OpenAIProvider and GeminiProvider**

**Commit:**
```bash
git add .
git commit -m "refactor: extract duplicate context building logic

- Add _build_context() helper to BaseLLMProvider
- Remove duplicate code from all 3 providers
- Reduces code by ~30 lines, improves maintainability"

git push
```

#### Day 3-5: Add Comprehensive Tests

**Create:** tests/test_vector_store.py
```python
import pytest
from unittest.mock import Mock, patch
from rag_system.core.retrieval.vector_store import ChromaVectorStore

class TestChromaVectorStore:
    def test_add_documents_validation(self):
        """Test input validation for add_documents"""
        store = ChromaVectorStore()

        # Test length mismatch
        with pytest.raises(ValueError, match="Length mismatch"):
            store.add_documents(
                texts=["a", "b"],
                metadatas=[{"key": "value"}],
                ids=["1"]
            )

        # Test empty inputs
        with pytest.raises(ValueError, match="cannot be empty"):
            store.add_documents(texts=[], metadatas=[], ids=[])

        # Test duplicate IDs
        with pytest.raises(ValueError, match="must be unique"):
            store.add_documents(
                texts=["a", "b"],
                metadatas=[{"k": "v"}, {"k": "v"}],
                ids=["1", "1"]
            )

    def test_collection_stats_caching(self):
        """Test that stats are cached correctly"""
        store = ChromaVectorStore()

        # First call - should hit DB
        stats1 = store.get_collection_stats()
        time1 = store._stats_cache_time

        # Second call immediately - should use cache
        stats2 = store.get_collection_stats()
        time2 = store._stats_cache_time

        assert stats1 == stats2
        assert time1 == time2  # Cache time unchanged

    def test_batch_size_calculation(self):
        """Test batch size calculation edge cases"""
        store = ChromaVectorStore()

        # Empty list
        assert store._calculate_optimal_batch_size([]) == 100

        # Very short texts
        short_texts = ["a" * 10 for _ in range(100)]
        batch_size = store._calculate_optimal_batch_size(short_texts)
        assert batch_size == 200  # Max batch size

        # Very long texts
        long_texts = ["a" * 10000 for _ in range(100)]
        batch_size = store._calculate_optimal_batch_size(long_texts)
        assert batch_size == 25  # Min batch size

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**Create:** tests/test_llm_handler.py
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from rag_system.core.generation.llm_handler import (
    LLMService, OllamaProvider, OpenAIProvider
)

class TestLLMService:
    def test_provider_fallback(self):
        """Test fallback to available provider"""
        service = LLMService()

        # Set to unavailable provider
        service.current_provider = 'nonexistent'

        with patch.object(service.providers.get('ollama', Mock()), 'is_available', return_value=True):
            with patch.object(service.providers.get('ollama', Mock()), 'generate_response', return_value="test response"):
                result = service.generate_answer("test question", [])

                assert result == "test response"
                assert service.current_provider == 'ollama'

    def test_context_building(self):
        """Test that context is built correctly"""
        provider = OllamaProvider()

        context = [
            {
                'content': 'Test content 1' * 100,
                'metadata': {'title': 'Doc 1'}
            },
            {
                'content': 'Test content 2' * 100,
                'metadata': {'title': 'Doc 2'}
            }
        ]

        context_text = provider._build_context(context, max_results=2, max_length=100)

        assert 'Doc 1' in context_text
        assert 'Doc 2' in context_text
        assert len(context_text) < 500  # Truncated properly

class TestOllamaProvider:
    def test_connection_pooling(self):
        """Test that session is created with pooling"""
        provider = OllamaProvider()

        assert hasattr(provider, 'session')
        assert isinstance(provider.session, requests.Session)

    @patch('requests.Session.post')
    def test_error_handling(self, mock_post):
        """Test error handling for failed requests"""
        provider = OllamaProvider()

        mock_post.side_effect = requests.Timeout()

        result = provider.generate_response("test", [])
        assert "Error" in result

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**Create:** tests/test_api.py
```python
import pytest
from fastapi.testclient import TestClient
from rag_system.api.server import create_app

@pytest.fixture
def client():
    app = create_app()
    return TestClient(app)

class TestAPIEndpoints:
    def test_root(self, client):
        """Test root endpoint"""
        response = client.get("/")
        assert response.status_code == 200
        assert "DocuMentor" in response.json()["message"]

    def test_status(self, client):
        """Test status endpoint"""
        response = client.get("/status")
        assert response.status_code == 200
        data = response.json()
        assert "status" in data
        assert "providers" in data

    def test_ask_question(self, client):
        """Test question endpoint"""
        payload = {"question": "What is FastAPI?"}
        response = client.post("/ask", json=payload)

        assert response.status_code in [200, 500]  # 500 if no LLM available

        if response.status_code == 200:
            data = response.json()
            assert "answer" in data
            assert "sources" in data

    def test_query_sanitization(self, client):
        """Test that dangerous characters are sanitized"""
        payload = {"question": "<script>alert('xss')</script>"}
        response = client.post("/ask", json=payload)

        # Should not crash, query should be sanitized
        assert response.status_code in [200, 500]

    def test_file_upload_size_limit(self, client):
        """Test file size limit enforcement"""
        large_file = ("test.txt", b"x" * (11 * 1024 * 1024), "text/plain")

        response = client.post("/upload", files={"file": large_file})

        assert response.status_code == 413  # Payload too large

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**Run tests:**
```bash
# Install test dependencies
pip install pytest pytest-cov pytest-asyncio

# Run all tests with coverage
pytest tests/ -v --cov=rag_system --cov-report=html --cov-report=term

# View coverage report
open htmlcov/index.html  # or xdg-open on Linux
```

**Commit:**
```bash
git add tests/
git commit -m "test: add comprehensive test suite

- Add vector store tests (validation, caching, batch sizing)
- Add LLM handler tests (fallback, context building, errors)
- Add API endpoint tests (sanitization, limits, errors)
- Current coverage: ~65% (target: 80%+)

Next: Add integration tests for full pipeline"

git push
```

---

### Week 4: Error Handling & Reliability

**Goal:** Make the system production-ready with robust error handling

#### Day 1-2: Add Proper Error Handling

**1. rag_system/core/generation/llm_handler.py**
```python
class OllamaProvider(BaseLLMProvider):
    def is_available(self) -> bool:
        """Check Ollama availability with specific error handling"""
        if not HAS_REQUESTS:
            logger.debug("requests library not available")
            return False

        try:
            response = self.session.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            return response.status_code == 200
        except requests.Timeout:
            logger.debug(f"Ollama connection timed out at {self.base_url}")
            return False
        except requests.ConnectionError:
            logger.debug(f"Cannot connect to Ollama at {self.base_url}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error checking Ollama: {type(e).__name__}: {e}")
            return False

class OpenAIProvider(BaseLLMProvider):
    def generate_response(self, prompt: str, context: List[Dict]) -> str:
        """Generate response with proper validation"""
        if not self.client:
            return "Error: OpenAI client not initialized"

        try:
            context_text = self._build_context(context)
            full_prompt = f"{context_text}\n\nQuestion: {prompt}\n\nAnswer:"

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful documentation assistant."},
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )

            # ‚≠ê VALIDATE response
            if not response or not response.choices:
                logger.error("OpenAI returned empty response")
                return "Error: No response generated"

            if not response.choices[0].message:
                logger.error("OpenAI response missing message")
                return "Error: Invalid response format"

            content = response.choices[0].message.content

            if not content:
                logger.warning("OpenAI returned empty content")
                return "Error: Empty response content"

            return content

        except openai.APIError as e:
            logger.error(f"OpenAI API error: {e}")
            return f"Error: OpenAI API issue - {str(e)[:100]}"
        except openai.RateLimitError as e:
            logger.error("OpenAI rate limit exceeded")
            return "Error: Rate limit exceeded. Please try again later."
        except Exception as e:
            logger.error(f"OpenAI generation failed: {type(e).__name__}: {e}")
            return f"Error: {type(e).__name__}"
```

**2. rag_system/core/retrieval/vector_store.py**
```python
def __init__(self):
    # ... existing init code ...

    try:
        self.collection = self.client.get_collection(
            name=self.collection_name
        )
        # ‚≠ê SAFE count with error handling
        try:
            doc_count = self.collection.count()
            logger.info(f"Loaded collection: {doc_count} docs")
        except Exception as e:
            logger.warning(f"Could not get collection count: {e}")
            logger.info("Loaded collection (count unavailable)")

    except ValueError:
        # Collection doesn't exist, create it
        # ... rest unchanged
```

**Commit:**
```bash
pytest tests/ -v

git add .
git commit -m "feat: add comprehensive error handling to LLM providers

- Add specific exception handling for each provider
- Validate OpenAI responses before returning
- Improve error messages for debugging
- Handle network errors gracefully in Ollama"

git push
```

---

### Week 5: Documentation & Final Polish

#### Create Development Documentation

**Create:** CONTRIBUTING.md
```markdown
# Contributing to DocuMentor

## Development Setup

```bash
# Clone and setup
git clone <repo>
cd DocuMentor
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Copy environment
cp .env.example .env
# Edit .env with your settings

# Run tests
pytest tests/ -v --cov=rag_system
```

## Code Standards

- Python 3.13+
- Type hints required for all functions
- Docstrings for all public methods
- Test coverage > 80%
- Max cyclomatic complexity: 10

## Before Committing

```bash
# Format code
black rag_system/ tests/

# Lint
flake8 rag_system/

# Type check
mypy rag_system/

# Test
pytest tests/ -v --cov=rag_system
```

## Commit Message Format

```
<type>: <subject>

<body>

<footer>
```

Types: feat, fix, docs, style, refactor, test, chore
```

**Commit:**
```bash
git add CONTRIBUTING.md REVERSE_ENGINEERING_GUIDE.md IMPROVEMENT_ROADMAP.md
git commit -m "docs: add comprehensive development documentation

- Add REVERSE_ENGINEERING_GUIDE.md for understanding codebase
- Add IMPROVEMENT_ROADMAP.md with implementation plan
- Add CONTRIBUTING.md for developers

These docs provide complete guide for new contributors"

git push
```

---

## üìù Quick Reference: Git Workflow

### For Each Improvement:

```bash
# 1. Create branch
git checkout -b <type>/<description>
# Examples:
#   fix/input-validation
#   perf/connection-pooling
#   test/vector-store

# 2. Make changes
# Edit files as described in roadmap

# 3. Test
pytest tests/ -v
# Fix any failures

# 4. Commit
git add .
git commit -m "<type>: <concise description>

<detailed description>
<impact/metrics>

Fixes: #issue-number"

# 5. Push
git push -u origin <branch-name>

# 6. Create PR (if using GitHub)
# Or merge directly if solo:
git checkout claude/codebase-overview-0Y0ig
git merge <branch-name>
git push
```

---

## üéØ Success Metrics

Track these after each week:

| Metric | Baseline | Week 1 | Week 2 | Week 3 | Week 4 | Target |
|--------|----------|---------|---------|---------|---------|---------|
| Security Issues | Unknown | 0 | 0 | 0 | 0 | 0 |
| Test Coverage | ~20% | 30% | 45% | 65% | 80% | 80%+ |
| API Response Time (p95) | Unknown | - | <500ms | <500ms | <300ms | <500ms |
| Code Complexity (avg) | Unknown | - | - | <8 | <6 | <6 |

---

## üîß Troubleshooting

### Tests Failing?
```bash
# Clear cache and reinstall
rm -rf .pytest_cache __pycache__ rag_system/__pycache__
pip install -r requirements.txt --force-reinstall

# Run with verbose output
pytest tests/ -v -s

# Run specific test
pytest tests/test_vector_store.py::TestChromaVectorStore::test_add_documents_validation -v
```

### Performance Not Improving?
```bash
# Profile the application
pip install py-spy
py-spy record -o profile.svg -- python main.py

# Or use cProfile
python -m cProfile -o output.prof api_server.py
python -m pstats output.prof
```

### Merge Conflicts?
```bash
# Pull latest
git pull origin main

# Resolve conflicts in editor
# Then:
git add .
git commit
git push
```

---

## üìö Additional Resources

- **FastAPI Best Practices**: https://github.com/zhanymkanov/fastapi-best-practices
- **Python Testing**: https://docs.pytest.org/
- **ChromaDB Docs**: https://docs.trychroma.com/
- **LangChain Guides**: https://python.langchain.com/docs/

---

**Ready to start? Begin with Week 1, Day 1 and work systematically through each task!**

Each improvement is designed to be:
- ‚úÖ Specific (exact files and lines)
- ‚úÖ Testable (with test cases)
- ‚úÖ Committable (with commit messages)
- ‚úÖ Measurable (with success metrics)
